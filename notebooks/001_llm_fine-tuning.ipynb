{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune an LLM to learn to count objects in a grid, or to solve ARC tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I might do 2 steps of fine-tuning:\n",
    "\n",
    "1. Learning priors, f.e. learning to count\n",
    "2. Solve ARC tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/ironbar/prompt_recovery/blob/main/notebooks/012_fine-tune_llama.ipynb\n",
    "- https://github.com/ironbar/prompt_recovery/blob/main/notebooks/020_fine-tune_final_ensemble.ipynb\n",
    "- https://www.kaggle.com/code/ironbar/few-shot-prompting-for-arc24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "from termcolor import colored\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import colors\n",
    "import wandb\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, pipeline\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from datasets import Dataset\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    model_path = \"/home/gbarbadillo/data/Phi-3-mini-128k-instruct\"\n",
    "    adapter_path: Optional[str] = '/mnt/hdd0/Kaggle/arc24/models/20240724_first_trainings/15_continue_training_phi3_4e5/checkpoint-22800' # Set it to None to train lora from scratch\n",
    "    train_dataset = '/mnt/hdd0/Kaggle/arc24/data/learn_to_count/learn_to_count_100000.json'\n",
    "    val_dataset = '/mnt/hdd0/Kaggle/arc24/data/learn_to_count/learn_to_count_1000.json'\n",
    "    output_dir = '/mnt/hdd0/Kaggle/arc24/models/20240724_first_trainings/22_random_question_lr1e-4_1e5dataset'\n",
    "    max_seq_len = 512\n",
    "    epochs = 1\n",
    "    eval_steps = 100\n",
    "    warmup_ratio = 0.1\n",
    "    learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    # model_path = \"/home/gbarbadillo/data/llama-3-transformers-8b-chat-hf-v1\"\n",
    "    model_path = '/home/gbarbadillo/data/llama-3.1-transformers-8b-instruct-v1'\n",
    "    adapter_path: Optional[str] = None\n",
    "    # adapter_path: Optional[str] = '/mnt/hdd0/Kaggle/arc24/models/20240724_first_trainings/12_llama_lr_2e-4_1e4dataset_r32/checkpoint-600'\n",
    "    use_rslora = False,\n",
    "    use_dora = False,\n",
    "    train_dataset = '/mnt/hdd0/Kaggle/arc24/data/learn_to_count/learn_to_count_100000.json'\n",
    "    val_dataset = '/mnt/hdd0/Kaggle/arc24/data/learn_to_count/learn_to_count_1000.json'\n",
    "    output_dir = '/mnt/hdd0/Kaggle/arc24/models/20240724_first_trainings/22_llama31_lr1e-4_1e5dataset_r32'\n",
    "    max_seq_len = 640\n",
    "    epochs = 1\n",
    "    eval_steps = 100\n",
    "    warmup_ratio = 0.1\n",
    "    learning_rate = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(cfg.output_dir, exist_ok=True)\n",
    "with open(os.path.join(cfg.output_dir, 'cfg.json'), 'w') as f:\n",
    "    json.dump({key:value for key, value in cfg.__dict__.items() if not key.startswith('__')}, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'llama' in cfg.model_path:\n",
    "    device_map = {\n",
    "        'model.embed_tokens': 0,\n",
    "        'model.layers.0': 0,\n",
    "        'model.layers.1': 0,\n",
    "        'model.layers.2': 0,\n",
    "        'model.layers.3': 0,\n",
    "        'model.layers.4': 0,\n",
    "        'model.layers.5': 0,\n",
    "        'model.layers.6': 0,\n",
    "        'model.layers.7': 0,\n",
    "        'model.layers.8': 0,\n",
    "        'model.layers.9': 0,\n",
    "        'model.layers.10': 0,\n",
    "        'model.layers.11': 0,\n",
    "        'model.layers.12': 0,\n",
    "        'model.layers.13': 0,\n",
    "        'model.layers.14': 0,\n",
    "        'model.layers.15': 0,\n",
    "        'model.layers.16': 0,\n",
    "        'model.layers.17': 1,\n",
    "        'model.layers.18': 1,\n",
    "        'model.layers.19': 1,\n",
    "        'model.layers.20': 1,\n",
    "        'model.layers.21': 1,\n",
    "        'model.layers.22': 1,\n",
    "        'model.layers.23': 1,\n",
    "        'model.layers.24': 1,\n",
    "        'model.layers.25': 1,\n",
    "        'model.layers.26': 1,\n",
    "        'model.layers.27': 1,\n",
    "        'model.layers.28': 1,\n",
    "        'model.layers.29': 1,\n",
    "        'model.layers.30': 1,\n",
    "        'model.layers.31': 1,\n",
    "        'model.norm': 1,\n",
    "        'model.rotary_emb': 1,\n",
    "        'lm_head': 1,\n",
    "    }\n",
    "else:\n",
    "    device_map = 'balanced'\n",
    "\n",
    "# device_map = 'balanced'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    cfg.model_path,\n",
    "    #quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    # max_memory={0: '9GB', 1: '8GB'},\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    cfg.model_path,\n",
    "    trust_remote_code=True)\n",
    "if 'llama' in cfg.model_path:\n",
    "    print('Adding <|pad|> token to tokenizer')\n",
    "    tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    tokenizer.padding_side = 'right'\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory():\n",
    "    for device in range(torch.cuda.device_count()):\n",
    "        print(f'GPU {device} memory allocated: {torch.cuda.memory_allocated(device)/1024**3:.1f} GB, max memory allocated: {torch.cuda.max_memory_allocated(device)/1024**3:.1f} GB')\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridEncoder(ABC):\n",
    "    @abstractmethod\n",
    "    def to_text(self, grid):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def to_grid(self, text):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_grid = np.eye(3, dtype=int).tolist()\n",
    "\n",
    "def test_translator(translator):\n",
    "    assert sample_grid == translator.to_grid(translator.to_text(sample_grid))\n",
    "    print(translator.to_text(sample_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimalGridEncoder(GridEncoder):\n",
    "    @staticmethod\n",
    "    def to_text(grid):\n",
    "        text = '\\n'.join([''.join([str(x) for x in line]) for line in grid])\n",
    "        return text\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_grid(text):\n",
    "        lines = text.strip().splitlines()\n",
    "        grid = [[int(x) for x in line] for line in lines]\n",
    "        return grid\n",
    "        \n",
    "test_translator(MinimalGridEncoder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWithSeparationEncoder(GridEncoder):\n",
    "    def __init__(self, split_symbol):\n",
    "        self.split_symbol = split_symbol\n",
    "\n",
    "    def to_text(self, grid):\n",
    "        text = '\\n'.join([self.split_symbol.join([str(x) for x in line]) for line in grid])\n",
    "        return text\n",
    "\n",
    "    def to_grid(self, text):\n",
    "        lines = text.strip().splitlines()\n",
    "        grid = [[int(x) for x in line.split(self.split_symbol)] for line in lines]\n",
    "        return grid\n",
    "\n",
    "test_translator(GridWithSeparationEncoder('|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridCodeBlockEncoder(GridEncoder):\n",
    "    def __init__(self, base_encoder):\n",
    "        self.encoder = base_encoder\n",
    "\n",
    "    def to_text(self, grid):\n",
    "        text = f'```grid\\n{self.encoder.to_text(grid)}\\n```'\n",
    "        return text\n",
    "\n",
    "    def to_grid(self, text):\n",
    "        grid_text = text.split('```grid\\n')[1].split('\\n```')[0]\n",
    "        grid = self.encoder.to_grid(grid_text)\n",
    "        return grid\n",
    "\n",
    "test_translator(GridCodeBlockEncoder(MinimalGridEncoder()))\n",
    "\n",
    "test_translator(GridCodeBlockEncoder(GridWithSeparationEncoder('|')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(filepath, grid_encoder, shuffle_question_order=True):\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    prompts = []\n",
    "\n",
    "    for sample_id, sample in tqdm(data.items(), total=len(data)):\n",
    "        messages = create_messages_from_sample(\n",
    "            sample, grid_encoder, shuffle_question_order=shuffle_question_order)\n",
    "        prompt = tokenizer.apply_chat_template(messages,\n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=False)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    np.random.shuffle(prompts)\n",
    "    pretty_print_prompt(prompts[0])\n",
    "\n",
    "    prompt_lengths = [len(tokenizer.encode(prompt)) for prompt in tqdm(prompts)]\n",
    "    plt.hist(prompt_lengths, bins=100);\n",
    "    plt.title('Prompt length distribution')\n",
    "    plt.xlabel('Number of tokens');\n",
    "    plt.show()\n",
    "\n",
    "    prompts = [prompt for prompt, prompt_length in zip(prompts, prompt_lengths) if prompt_length < cfg.max_seq_len]\n",
    "    print(f'Using {len(prompts)} prompts after removing those longer than {cfg.max_seq_len} tokens')\n",
    "\n",
    "    dataset = Dataset.from_dict({'text': prompts})\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def create_messages_from_sample(sample, grid_encoder, shuffle_question_order=False):\n",
    "    first_message = True\n",
    "    messages = [{'role': 'system', 'content': 'You are a helpful AI assistant'}]\n",
    "    questions = list(sample['questions'].keys())\n",
    "    if shuffle_question_order:\n",
    "        np.random.shuffle(questions)\n",
    "    for question in questions:\n",
    "        answer = sample['questions'][question]\n",
    "        if first_message:\n",
    "            content = grid_encoder.to_text(sample['grid']) + '\\n' + question\n",
    "            first_message = False\n",
    "        else:\n",
    "            content = question\n",
    "        messages.append({'role': 'user', 'content': content})\n",
    "        messages.append({'role': 'assistant', 'content': str(answer)})\n",
    "    return messages\n",
    "\n",
    "\n",
    "def pretty_print_prompt(text, default_color='white'):\n",
    "    color = default_color\n",
    "    attrs = None\n",
    "    for line in text.splitlines():\n",
    "        if line.startswith('<|assistant|>'):\n",
    "            color = 'blue'\n",
    "        elif line.startswith('<|user|>'):\n",
    "            color = default_color\n",
    "        elif line.startswith('<|system|>'):\n",
    "            color = 'green'\n",
    "        if line.startswith('<'):\n",
    "            attrs = ['bold']\n",
    "        else:\n",
    "            attrs = None\n",
    "        print(colored(line, color, attrs=attrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'llama' in cfg.model_path:\n",
    "    # we need to add separation between numbers in the grid\n",
    "    grid_encoder = GridCodeBlockEncoder(GridWithSeparationEncoder('|'))\n",
    "else:\n",
    "    grid_encoder = GridCodeBlockEncoder(MinimalGridEncoder())\n",
    "train_dataset = create_dataset(cfg.train_dataset, grid_encoder, shuffle_question_order=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = create_dataset(cfg.val_dataset, grid_encoder, shuffle_question_order=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.adapter_path is None:\n",
    "    peft_config = LoraConfig(\n",
    "        # lora_alpha: LoRA scaling factor.\n",
    "        lora_alpha=64, #64,\n",
    "        lora_dropout=0.1, # 0.1, althought Vaca suggested to use 0.05 for big models\n",
    "        # r: the rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters.\n",
    "        r=32, #16\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        # target_modules: The modules (for example, attention blocks) to apply the LoRA update matrices.\n",
    "        target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj'],\n",
    "        use_rslora=cfg.use_rslora,\n",
    "        use_dora=cfg.use_dora,\n",
    "    )\n",
    "else:\n",
    "    print(f'Loading adapter from {cfg.adapter_path}')\n",
    "    peft_config = None\n",
    "    model = PeftModel.from_pretrained(model, cfg.adapter_path, is_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'llama' in cfg.model_path:\n",
    "    batch_size_kwargs = dict(\n",
    "        per_device_train_batch_size=3, # 4-16 should be fine for lora.\n",
    "        gradient_accumulation_steps=5,\n",
    "        per_device_eval_batch_size=4,\n",
    "    )\n",
    "else:\n",
    "    batch_size_kwargs = dict(\n",
    "        per_device_train_batch_size=8, # 4-16 should be fine for lora.\n",
    "        gradient_accumulation_steps=2,\n",
    "        per_device_eval_batch_size=8,\n",
    "    )\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "        output_dir=cfg.output_dir,\n",
    "        num_train_epochs=cfg.epochs,\n",
    "        warmup_ratio=cfg.warmup_ratio,\n",
    "        learning_rate=cfg.learning_rate,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "\n",
    "        do_eval=True,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_steps=cfg.eval_steps,\n",
    "        logging_steps=10, #50,\n",
    "        eval_steps=cfg.eval_steps,\n",
    "        log_level=\"debug\",\n",
    "\n",
    "        **batch_size_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'llama' in cfg.model_path:\n",
    "    data_collator = DataCollatorForCompletionOnlyLM(\n",
    "        tokenizer=tokenizer,\n",
    "        instruction_template='<|start_header_id|>user<|end_header_id|>',\n",
    "        response_template='<|start_header_id|>assistant<|end_header_id|>',\n",
    "    )\n",
    "else:\n",
    "    data_collator = DataCollatorForCompletionOnlyLM(\n",
    "        tokenizer=tokenizer,\n",
    "        instruction_template='<|user|>',\n",
    "        response_template='<|assistant|>'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = wandb.init(reinit=True,\n",
    "               dir=cfg.output_dir,\n",
    "               project=os.path.basename(os.path.dirname(cfg.output_dir)),\n",
    "               name=os.path.basename(cfg.output_dir))\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=cfg.max_seq_len,\n",
    "    data_collator=data_collator,\n",
    "    args=training_arguments,\n",
    "    # packing=True, # ValueError: You passed a `DataCollatorForCompletionOnlyLM` to the SFTTrainer. This is not compatible with the `packing` argument.\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "w.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluation for 1k samples is taking around 1 minute.\n",
    "- One epoch has taken around 30 minutes, with around 11 evaluations. Thus 1/3 of the time was spend evaluating. (550 steps, eval every 50 steps)\n",
    "- Training for 10 epochs took 5 hours. I set the temperature of the AC to 27ºC and the room was at 22ºC, probably 28ºC is fine.\n",
    "- Eval loss was 0.1665, without signs of overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "300 steps without flash attention: 16:17 minutes, with flash attention 14 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25 minutes to do 620 steps, around 10k samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://huggingface.co/docs/transformers/en/peft\n",
    "- https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.merge_and_unload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cfg.val_dataset, 'r') as f:\n",
    "    data = json.load(f)\n",
    "val_samples_ids = list(data.keys())\n",
    "\n",
    "def ask_question_to_model(sample_idx, pipe, question_idx=0, arbitrary_question=None):\n",
    "    sample_id = val_samples_ids[sample_idx]\n",
    "    sample = data[sample_id]\n",
    "\n",
    "    sample_with_one_question = sample.copy()\n",
    "    if arbitrary_question is None:\n",
    "        sample_with_one_question['questions'] = {question:answer for idx, (question, answer) in enumerate(sample['questions'].items()) if idx == question_idx}\n",
    "    else:\n",
    "        sample_with_one_question['questions'] = {arbitrary_question:''}\n",
    "\n",
    "    messages = create_messages_from_sample(sample_with_one_question, grid_encoder)\n",
    "    prompt = tokenizer.apply_chat_template(messages[:2],\n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=True)\n",
    "    plot_grid(sample['grid']); plt.show()\n",
    "    # pretty_print_prompt(prompt)\n",
    "\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 50,\n",
    "        \"return_full_text\": False,\n",
    "        \"do_sample\": False,\n",
    "    }\n",
    "\n",
    "    output = pipe(prompt, **generation_args)\n",
    "    print(list(sample_with_one_question['questions'].keys())[0])\n",
    "    print(f\">{output[0]['generated_text']} ({list(sample_with_one_question['questions'].values())[0]})\")\n",
    "\n",
    "def plot_grid(grid):\n",
    "    grid = np.array(grid)\n",
    "    cmap = colors.ListedColormap(\n",
    "        ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n",
    "         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n",
    "    norm = colors.Normalize(vmin=0, vmax=9)\n",
    "    plt.imshow(grid, cmap=cmap, norm=norm)\n",
    "    plt.grid(True,which='both',color='lightgrey', linewidth=0.5)\n",
    "    plt.xticks(np.arange(-0.5, grid.shape[1]), [])\n",
    "    plt.yticks(np.arange(-0.5, grid.shape[0]), [])\n",
    "    plt.xlim(-0.5, grid.shape[1]-0.5)\n",
    "\n",
    "    for i in range(grid.shape[0]):\n",
    "        for j in range(grid.shape[1]):\n",
    "            plt.text(j, i, grid[i, j], ha='center', va='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = '/mnt/hdd0/Kaggle/arc24/models/20240724_first_trainings/11_lr_4e-4_1e5dataset_r32/checkpoint-12400'\n",
    "adapter_path = '/mnt/hdd0/Kaggle/arc24/models/20240724_first_trainings/09_lr_1e-3_1e4dataset_r32/checkpoint-600/'\n",
    "adapter_path = '/mnt/hdd0/Kaggle/arc24/models/20240724_first_trainings/14_llama31/checkpoint-333'\n",
    "adapter_path = '/mnt/hdd0/Kaggle/arc24/models/20240724_first_trainings/15_continue_training_phi3_4e5/checkpoint-22800'\n",
    "adapter_path = '/mnt/hdd0/Kaggle/arc24/models/20240724_first_trainings/22_random_question_lr1e-4_1e5dataset/checkpoint-6228'\n",
    "adapter_path = '/mnt/hdd0/Kaggle/arc24/models/20240724_first_trainings/22_llama31_lr1e-4_1e5dataset_r32/checkpoint-6553'\n",
    "model.load_adapter(adapter_path, adapter_path)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to visualize the grid and ask some random question, see how well it does.\n",
    "\n",
    "Compare the responses with and without the adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_question_to_model(sample_idx=750, question_idx=1, pipe=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask_question_to_model(sample_idx=300, arbitrary_question='Please describe the grid, saying how many objects are there, their color and area.', pipe=pipe)\n",
    "# ask_question_to_model(sample_idx=300, arbitrary_question='What is the shape of the grid? (nxn)', pipe=pipe)\n",
    "ask_question_to_model(sample_idx=550, arbitrary_question='Describe the objects in the grid', pipe=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] Fixed val dataset\n",
    "- [x] The pad_token_id and eos_token_id values of this tokenizer are identical. If you are planning for multi-turn training, it can result in the model continuously generating questions and answers without eos token. To avoid this, set the pad_token_id to a different value. **I have evaluated phi-3 and works correctly**\n",
    "- [x] Flash attention? Yes, it is slightly faster.\n",
    "- [x] Batch size\n",
    "- [x] Better WANDB configuration\n",
    "- [x] Training parameters?\n",
    "  - [x] Learning rate\n",
    "  - [x] Is the linear decay working properly? Yes\n",
    "- [x] Better switch between Llama and Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prometeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
